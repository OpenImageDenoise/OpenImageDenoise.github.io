<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html" charset="utf-8">
    <title>Intel® Open Image Denoise</title>
    <link rel="stylesheet" type="text/css" media="screen" href="stylesheet.css">
    <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.sourceCode { overflow-x: initial; }
    </style>
  </head>

  <body>
    <div id="header">
      <div id="header-github">
        <a id="forkme-banner" href="https://github.com/OpenImageDenoise/oidn">View on GitHub</a>
      </div>
      <div id="header-title">
        Intel<sup>®</sup> Open Image Denoise
      </div>

      <div id="header-navbar">
        <ul>
          <li><a href="index.html">Overview</a></li>
	  <li id="selected"><a href="documentation.html">Documentation</a></li>
	  <li><a href="gallery.html">Gallery</a></li>
	  <li><a href="downloads.html">Downloads</a></li>
	  <li><a href="https://github.com/OpenImageDenoise/oidn/issues">Bugs/Issues</a></li>
	  <li><a href="related_projects.html">Related Projects</a></li>
	</ul>
      </div>
      <div id="header-spacing"></div>
    </div>

    <div id="content-wrap">
      <div id="content">

<h1 id="documentation">Documentation</h1>
<p>The following <a href="https://github.com/OpenImageDenoise/oidn/blob/master/readme.pdf" title="Intel Open Image Denoise Documentation">documentation</a> of Intel Open Image Denoise can also be found as a <a href="https://github.com/OpenImageDenoise/oidn/blob/master/readme.pdf" title="Intel Open Image Denoise Documentation">pdf document</a>.</p>
<h1 id="open-image-denoise-api">Open Image Denoise API</h1>
<p>Open Image Denoise provides a C99 API (also compatible with C++) and a C++11 wrapper API as well. For simplicity, this document mostly refers to the C99 version of the API.</p>
<p>The API is designed in an object-oriented manner, e.g. it contains device objects (<code>OIDNDevice</code> type), buffer objects (<code>OIDNBuffer</code> type), and filter objects (<code>OIDNFilter</code> type). All objects are reference-counted, and handles can be released by calling the appropriate release function (e.g. <code>oidnReleaseDevice</code>) or retained by incrementing the reference count (e.g. <code>oidnRetainDevice</code>).</p>
<p>An important aspect of objects is that setting their parameters do not have an immediate effect (with a few exceptions). Instead, objects with updated parameters are in an unusable state until the parameters get explicitly committed to a given object. The commit semantic allows for batching up multiple small changes, and specifies exactly when changes to objects will occur.</p>
<p>All API calls are thread-safe, but operations that use the same device will be serialized, so the amount of API calls from different threads should be minimized.</p>
<h2 id="examples">Examples</h2>
<p>To have a quick overview of the C99 and C++11 APIs, see the following simple example code snippets.</p>
<h3 id="basic-denoising-c99-api">Basic Denoising (C99 API)</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb1-1"><a href="#cb1-1"></a><span class="pp">#include </span><span class="im">&lt;OpenImageDenoise/oidn.h&gt;</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>...</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co">// Create an Open Image Denoise device</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>OIDNDevice device = oidnNewDevice(OIDN_DEVICE_TYPE_DEFAULT); <span class="co">// CPU or GPU if available</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>oidnCommitDevice(device);</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">// Create buffers for input/output images accessible by both host (CPU) and device (CPU/GPU)</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>OIDNBuffer colorBuf  = oidnNewBuffer(device, width * height * <span class="dv">3</span> * <span class="kw">sizeof</span>(<span class="dt">float</span>));</span>
<span id="cb1-10"><a href="#cb1-10"></a>OIDNBuffer albedoBuf = ...</span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co">// Create a filter for denoising a beauty (color) image using optional auxiliary images too</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="co">// This can be an expensive operation, so try not to create a new filter for every image!</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>OIDNFilter filter = oidnNewFilter(device, <span class="st">&quot;RT&quot;</span>); <span class="co">// generic ray tracing filter</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>oidnSetFilterImage(filter, <span class="st">&quot;color&quot;</span>,  colorBuf,</span>
<span id="cb1-16"><a href="#cb1-16"></a>                   OIDN_FORMAT_FLOAT3, width, height, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>); <span class="co">// beauty</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>oidnSetFilterImage(filter, <span class="st">&quot;albedo&quot;</span>, albedoBuf,</span>
<span id="cb1-18"><a href="#cb1-18"></a>                   OIDN_FORMAT_FLOAT3, width, height, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>); <span class="co">// auxiliary</span></span>
<span id="cb1-19"><a href="#cb1-19"></a>oidnSetFilterImage(filter, <span class="st">&quot;normal&quot;</span>, normalBuf,</span>
<span id="cb1-20"><a href="#cb1-20"></a>                   OIDN_FORMAT_FLOAT3, width, height, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>); <span class="co">// auxiliary</span></span>
<span id="cb1-21"><a href="#cb1-21"></a>oidnSetFilterImage(filter, <span class="st">&quot;output&quot;</span>, colorBuf,</span>
<span id="cb1-22"><a href="#cb1-22"></a>                   OIDN_FORMAT_FLOAT3, width, height, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>); <span class="co">// denoised beauty</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>oidnSetFilterBool(filter, <span class="st">&quot;hdr&quot;</span>, <span class="kw">true</span>); <span class="co">// beauty image is HDR</span></span>
<span id="cb1-24"><a href="#cb1-24"></a>oidnCommitFilter(filter);</span>
<span id="cb1-25"><a href="#cb1-25"></a></span>
<span id="cb1-26"><a href="#cb1-26"></a><span class="co">// Fill the input image buffers</span></span>
<span id="cb1-27"><a href="#cb1-27"></a><span class="dt">float</span>* colorPtr = (<span class="dt">float</span>*)oidnGetBufferData(colorBuf);</span>
<span id="cb1-28"><a href="#cb1-28"></a>...</span>
<span id="cb1-29"><a href="#cb1-29"></a></span>
<span id="cb1-30"><a href="#cb1-30"></a><span class="co">// Filter the beauty image</span></span>
<span id="cb1-31"><a href="#cb1-31"></a>oidnExecuteFilter(filter);</span>
<span id="cb1-32"><a href="#cb1-32"></a></span>
<span id="cb1-33"><a href="#cb1-33"></a><span class="co">// Check for errors</span></span>
<span id="cb1-34"><a href="#cb1-34"></a><span class="at">const</span> <span class="dt">char</span>* errorMessage;</span>
<span id="cb1-35"><a href="#cb1-35"></a><span class="cf">if</span> (oidnGetDeviceError(device, &amp;errorMessage) != OIDN_ERROR_NONE)</span>
<span id="cb1-36"><a href="#cb1-36"></a>  printf(<span class="st">&quot;Error: </span><span class="sc">%s\n</span><span class="st">&quot;</span>, errorMessage);</span>
<span id="cb1-37"><a href="#cb1-37"></a></span>
<span id="cb1-38"><a href="#cb1-38"></a><span class="co">// Cleanup</span></span>
<span id="cb1-39"><a href="#cb1-39"></a>oidnReleaseBuffer(colorBuf);</span>
<span id="cb1-40"><a href="#cb1-40"></a>...</span>
<span id="cb1-41"><a href="#cb1-41"></a>oidnReleaseFilter(filter);</span>
<span id="cb1-42"><a href="#cb1-42"></a>oidnReleaseDevice(device);</span></code></pre></div>
<h3 id="basic-denoising-c11-api">Basic Denoising (C++11 API)</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1"></a><span class="pp">#include </span><span class="im">&lt;OpenImageDenoise/oidn.hpp&gt;</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>...</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co">// Create an Open Image Denoise device</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>oidn::DeviceRef device = oidn::newDevice(); <span class="co">// CPU or GPU if available</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>device.commit();</span>
<span id="cb2-7"><a href="#cb2-7"></a></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co">// Create buffers for input/output images accessible by both host (CPU) and device (CPU/GPU)</span></span>
<span id="cb2-9"><a href="#cb2-9"></a>oidn::BufferRef colorBuf  = device.newBuffer(width * height * <span class="dv">3</span> * <span class="kw">sizeof</span>(<span class="dt">float</span>));</span>
<span id="cb2-10"><a href="#cb2-10"></a>oidn::BufferRef albedoBuf = ...</span>
<span id="cb2-11"><a href="#cb2-11"></a></span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="co">// Create a filter for denoising a beauty (color) image using optional auxiliary images too</span></span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="co">// This can be an expensive operation, so try no to create a new filter for every image!</span></span>
<span id="cb2-14"><a href="#cb2-14"></a>oidn::FilterRef filter = device.newFilter(<span class="st">&quot;RT&quot;</span>); <span class="co">// generic ray tracing filter</span></span>
<span id="cb2-15"><a href="#cb2-15"></a>filter.setImage(<span class="st">&quot;color&quot;</span>,  colorBuf,  oidn::Format::Float3, width, height); <span class="co">// beauty</span></span>
<span id="cb2-16"><a href="#cb2-16"></a>filter.setImage(<span class="st">&quot;albedo&quot;</span>, albedoBuf, oidn::Format::Float3, width, height); <span class="co">// auxiliary</span></span>
<span id="cb2-17"><a href="#cb2-17"></a>filter.setImage(<span class="st">&quot;normal&quot;</span>, normalBuf, oidn::Format::Float3, width, height); <span class="co">// auxiliary</span></span>
<span id="cb2-18"><a href="#cb2-18"></a>filter.setImage(<span class="st">&quot;output&quot;</span>, colorBuf,  oidn::Format::Float3, width, height); <span class="co">// denoised beauty</span></span>
<span id="cb2-19"><a href="#cb2-19"></a>filter.set(<span class="st">&quot;hdr&quot;</span>, <span class="kw">true</span>); <span class="co">// beauty image is HDR</span></span>
<span id="cb2-20"><a href="#cb2-20"></a>filter.commit();</span>
<span id="cb2-21"><a href="#cb2-21"></a></span>
<span id="cb2-22"><a href="#cb2-22"></a><span class="co">// Fill the input image buffers</span></span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="dt">float</span>* colorPtr = (<span class="dt">float</span>*)colorBuf.getData();</span>
<span id="cb2-24"><a href="#cb2-24"></a>...</span>
<span id="cb2-25"><a href="#cb2-25"></a></span>
<span id="cb2-26"><a href="#cb2-26"></a><span class="co">// Filter the beauty image</span></span>
<span id="cb2-27"><a href="#cb2-27"></a>filter.execute();</span>
<span id="cb2-28"><a href="#cb2-28"></a></span>
<span id="cb2-29"><a href="#cb2-29"></a><span class="co">// Check for errors</span></span>
<span id="cb2-30"><a href="#cb2-30"></a><span class="at">const</span> <span class="dt">char</span>* errorMessage;</span>
<span id="cb2-31"><a href="#cb2-31"></a><span class="cf">if</span> (device.getError(errorMessage) != oidn::Error::None)</span>
<span id="cb2-32"><a href="#cb2-32"></a>  <span class="bu">std::</span>cout &lt;&lt; <span class="st">&quot;Error: &quot;</span> &lt;&lt; errorMessage &lt;&lt; <span class="bu">std::</span>endl;</span></code></pre></div>
<h3 id="denoising-with-prefiltering-c11-api">Denoising with Prefiltering (C++11 API)</h3>
<div class="sourceCode" id="cb3"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb3-1"><a href="#cb3-1"></a><span class="co">// Create a filter for denoising a beauty (color) image using prefiltered auxiliary images too</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>oidn::FilterRef filter = device.newFilter(<span class="st">&quot;RT&quot;</span>); <span class="co">// generic ray tracing filter</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>filter.setImage(<span class="st">&quot;color&quot;</span>,  colorBuf,  oidn::Format::Float3, width, height); <span class="co">// beauty</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>filter.setImage(<span class="st">&quot;albedo&quot;</span>, albedoBuf, oidn::Format::Float3, width, height); <span class="co">// auxiliary</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>filter.setImage(<span class="st">&quot;normal&quot;</span>, normalBuf, oidn::Format::Float3, width, height); <span class="co">// auxiliary</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>filter.setImage(<span class="st">&quot;output&quot;</span>, outputBuf, oidn::Format::Float3, width, height); <span class="co">// denoised beauty</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>filter.set(<span class="st">&quot;hdr&quot;</span>, <span class="kw">true</span>); <span class="co">// beauty image is HDR</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>filter.set(<span class="st">&quot;cleanAux&quot;</span>, <span class="kw">true</span>); <span class="co">// auxiliary images will be prefiltered</span></span>
<span id="cb3-9"><a href="#cb3-9"></a>filter.commit();</span>
<span id="cb3-10"><a href="#cb3-10"></a></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co">// Create a separate filter for denoising an auxiliary albedo image (in-place)</span></span>
<span id="cb3-12"><a href="#cb3-12"></a>oidn::FilterRef albedoFilter = device.newFilter(<span class="st">&quot;RT&quot;</span>); <span class="co">// same filter type as for beauty</span></span>
<span id="cb3-13"><a href="#cb3-13"></a>albedoFilter.setImage(<span class="st">&quot;albedo&quot;</span>, albedoBuf, oidn::Format::Float3, width, height);</span>
<span id="cb3-14"><a href="#cb3-14"></a>albedoFilter.setImage(<span class="st">&quot;output&quot;</span>, albedoBuf, oidn::Format::Float3, width, height);</span>
<span id="cb3-15"><a href="#cb3-15"></a>albedoFilter.commit();</span>
<span id="cb3-16"><a href="#cb3-16"></a></span>
<span id="cb3-17"><a href="#cb3-17"></a><span class="co">// Create a separate filter for denoising an auxiliary normal image (in-place)</span></span>
<span id="cb3-18"><a href="#cb3-18"></a>oidn::FilterRef normalFilter = device.newFilter(<span class="st">&quot;RT&quot;</span>); <span class="co">// same filter type as for beauty</span></span>
<span id="cb3-19"><a href="#cb3-19"></a>normalFilter.setImage(<span class="st">&quot;normal&quot;</span>, normalBuf, oidn::Format::Float3, width, height);</span>
<span id="cb3-20"><a href="#cb3-20"></a>normalFilter.setImage(<span class="st">&quot;output&quot;</span>, normalBuf, oidn::Format::Float3, width, height);</span>
<span id="cb3-21"><a href="#cb3-21"></a>normalFilter.commit();</span>
<span id="cb3-22"><a href="#cb3-22"></a></span>
<span id="cb3-23"><a href="#cb3-23"></a><span class="co">// Prefilter the auxiliary images</span></span>
<span id="cb3-24"><a href="#cb3-24"></a>albedoFilter.execute();</span>
<span id="cb3-25"><a href="#cb3-25"></a>normalFilter.execute();</span>
<span id="cb3-26"><a href="#cb3-26"></a></span>
<span id="cb3-27"><a href="#cb3-27"></a><span class="co">// Filter the beauty image</span></span>
<span id="cb3-28"><a href="#cb3-28"></a>filter.execute();</span></code></pre></div>
<h2 id="upgrading-from-open-image-denoise-1.x">Upgrading from Open Image Denoise 1.x</h2>
<p>Open Image Denoise 2 introduces GPU support, which requires implementing some minor changes in applications. There are also small API changes, additions and improvements in this new version. In this section we summarize the necessary code modifications and also briefly mention the new features that users might find useful when upgrading to version 2.x. For a full description of the changes and new functionality, please see the API reference.</p>
<h3 id="buffers">Buffers</h3>
<p>The most important required change is related to how data is passed to Open Image Denoise. If the application is explicitly using only the CPU (by specifying <code>OIDN_DEVICE_TYPE_CPU</code>), no changes should be necessary. But if it wants to support GPUs as well, passing pointers to memory allocated with the system allocator (e.g. <code>malloc</code>) would raise an error because GPUs cannot access such memory in almost all cases.</p>
<p>To ensure compatibility with any kind of device, including GPUs, the application should use <code>OIDNBuffer</code> objects to store all image data passed to the library. Memory allocated using buffers is by default accessible by both the host (CPU) and the device (CPU or GPU).</p>
<p>Ideally, the application should directly read and write image data to/from such buffers to avoid redundant and inefficient data copying. If this cannot be implemented, the application should try to minimize the overhead of copying as much as possible:</p>
<ul>
<li><p>Data should be copied to/from buffers only if the data in system memory indeed cannot be accessed by the device. This can be determined by simply querying the <code>systemMemorySupported</code> device parameter. If system memory is accessible by the device, no buffers are necessary and filter image parameters can be set with <code>oidnSetSharedFilterImage</code>.</p></li>
<li><p>If the image data cannot be accessed by the device, buffers must be created and the data must be copied to/from these buffers. These buffers should be directly passed to filters as image parameters instead of the original pointers using <code>oidnSetFilterImage</code>.</p></li>
<li><p>Data should be copied asynchronously using using the new <code>oidnReadBufferAsync</code> and <code>oidnWriteBufferAsync</code> functions, which may achieve higher performance than plain <code>memcpy</code>.</p></li>
<li><p>If image data must be copied, using the default buffer allocation may not be the most efficient method. If the device memory is not physically shared with the host memory (e.g. for dedicated GPUs), higher performance may be achieved by creating the buffers with device storage (<code>OIDN_STORAGE_DEVICE</code>) using the new <code>oidnNewBufferWithStorage</code> function. This way, the buffer data cannot be directly accessed by the host anymore but this should not matter because the data must be copied from some other memory location anyway. However, this ensures that the data is stored only in high-performance device memory, and the user has full control over when and how the data is transferred between host and device.</p></li>
</ul>
<p>The <code>oidnMapBuffer</code> and <code>oidnUnmapBuffer</code> functions have been removed from the API due to these not being supported by any of the device backends. Please use <code>oidnReadBuffer(Async)</code> and <code>oidnWriteBuffer(Async)</code> instead.</p>
<h3 id="interop-with-compute-sycl-cuda-hip-and-graphics-dx-vulkan-apis">Interop with Compute (SYCL, CUDA, HIP) and Graphics (DX, Vulkan) APIs</h3>
<p>If the application is explicitly using a particular device type which supports unified memory allocations, e.g. SYCL or CUDA, it may directly pass pointers allocated using the native allocator of the respective compute API (e.g. <code>sycl::malloc_device</code>, <code>cudaMalloc</code>) instead of using buffers. This way, it is the responsibility of the user to correctly allocate the memory for the device.</p>
<p>In such cases, it often necessary to have more control over the device creation as well, to ensure that filtering is running on the intended device and command queues or streams from the application can be shared to improve performance. If the application is using the same compute or graphics API as the Open Image Denoise device, this can be achieved by creating devices with <code>oidnNewSYCLDevice</code>, <code>oidnNewCUDADevice</code>, etc. For some APIs there are additional interoperability functions as well, e.g. <code>oidnExecuteSYCLFilterAsync</code>.</p>
<p>If the application is using a graphics API which does not support unified memory allocations, e.g. DX12 or Vulkan, it may be still possible to share memory between the application and Open Image Denoise using buffers, avoiding expensive copying through host memory. External buffers can be imported from graphics APIs with the new <code>oidnNewSharedBufferFromFD</code> and <code>oidnNewSharedBufferFromWin32Handle</code> functions. To use this feature, buffers must be exported in the graphics API and must be imported in Open Image Denoise using the same kind of handle. Care must be taken to select an external memory handle type which is supported by both APIs. The external memory types supported by an Open Image Denoise device can be queried using the <code>externalMemoryTypes</code> device parameter. Note that some devices do not support importing external memory at all (e.g. CPUs, and on GPUs it primarily depends on the installed drivers), so the application should always implement a fallback too, which copies the data through the host if there is no other supported way.</p>
<p>Sharing textures is currently not supported natively but it is still possible avoid copying texture data by using a linear texture layout (e.g. <code>VK_IMAGE_TILING_LINEAR</code> in Vulkan) and sharing the buffer that backs this data. In this case, you should ensure that the row stride of the linear texture data is correctly set.</p>
<p>Importing external synchronization primitives (e.g. semaphores) from graphics APIs is not yet supported either but it is planned for a future release. Meanwhile, synchronizing access to shared memory should be done on the host using <code>oidnSyncDevice</code> and the used graphics API.</p>
<p>When importing external memory, the application also needs to make sure that the Open Image Denoise device is running on the same <em>physical</em> device as the graphics API. This can be easily achieved by using the new physical device feature, described in the next section.</p>
<h3 id="physical-devices">Physical Devices</h3>
<p>Although it is possible to explicitly create devices of a particular type (with, e.g., <code>OIDN_DEVICE_TYPE_SYCL</code>), this is often insufficient, especially if the system has multiple devices of the same type, and with GPU support it is very common that there are multiple different types of supported devices in the system (e.g. a CPU and one or more GPUs).</p>
<p>Open Image Denoise 2 introduces a simple <em>physical device</em> API, which enables the application to query the list of supported physical devices in the system, including their name, type, UUID, LUID, PCI address, etc. (see <code>oidnGetNumPhysicalDevices</code>, <code>oidnGetPhysicalDeviceString</code>, etc.). New logical device (i.e. <code>OIDNDevice</code>) creation functions for have been also introduced, which enable creating a logical device on a specific physical device: <code>oidnNewDeviceByID</code>, <code>oidnNewDeviceByUUID</code>, etc.</p>
<p>Creating a logical device on a physical device having a particular UUID, LUID or PCI address is particularly important when importing external memory from graphics APIs. However, not all device types support all types of IDs, and some graphics drivers may even report mismatching UUIDs or LUIDs for the same physical device, so applications should try to implement multiple identification methods, or at least assume that identification might fail.</p>
<h3 id="asynchronous-execution">Asynchronous Execution</h3>
<p>With the introduction of GPU support, it is now possible to execute some operations asynchronously, most importantly filtering (<code>oidnExecuteFilterAsync</code>, <code>oidnExecuteSYCLFilterAsync</code>) and copying data (the already mentioned <code>oidnReadBufferAsync</code> and <code>oidnWriteBufferAsync</code>). Although these new asynchronous functions can be used with any device type, it is <em>not</em> guaranteed that these will be actually executed asynchronously. The most important such exceptions are CPU devices, which are still blocking the calling thread when these functions are called.</p>
<p>When using any asynchronous function it is the responsibility of the application to handle correct synchronization using <code>oidnSyncDevice</code>.</p>
<h3 id="filter-quality">Filter Quality</h3>
<p>Open Image Denoise still delivers the same high image quality on all device types as before, including on GPUs. But often filtering performance is more important than having the highest possible image quality, so it is now possible to switch between multiple filter quality modes. Filters have a new parameter called <code>quality</code>, which defaults to the existing high image quality (<code>OIDN_QUALITY_HIGH</code>) but a balanced quality mode (<code>OIDN_QUALITY_BALANCED</code>) has been added as well for even higher performance. We recommend using balanced quality for interactive and real-time use cases.</p>
<h3 id="small-api-changes">Small API Changes</h3>
<p>A few existing API functions have been renamed to improve clarity (e.g. <code>oidnSetFilter1i</code> to <code>oidnSetFilterInt</code>) but the old function names are still available as deprecated functions. When compiling legacy code, warnings will be emitted for these deprecated functions. To upgrade to the new API, please simply follow the instructions in the warnings.</p>
<p>Some filter parameters have been also renamed (<code>alignment</code> to <code>tileAlignment</code>, <code>overlap</code> to <code>tileOverlap</code>). When using the old names, warnings will be emitted at runtime.</p>
<h3 id="building-as-a-static-library">Building as a Static Library</h3>
<p>The support to build Open Image Denoise as a static library (<code>OIDN_STATIC_LIB</code> CMake option) has been limited to CPU-only builds due to switching to a modular library design that was necessary for adding multi-vendor GPU support. If the library is built with GPU support as well, the <code>OIDN_STATIC_LIB</code> option is still available but enabling it results in a hybrid static/shared library.</p>
<p>If the main reason for building as a static library would be is the ability to use multiple versions of Open Image Denoise in the same process, please use the existing <code>OIDN_API_NAMESPACE</code> CMake option instead. With this feature all symbols of the library will be put into a custom namespace, which can prevent symbol clashes.</p>
<h2 id="physical-devices-1">Physical Devices</h2>
<p>Systems often have multiple different types of devices supported by Open Image Denoise (CPUs and GPUs). The application can get the list of supported <em>physical devices</em> and select which of these to use for denoising.</p>
<p>The number of supported physical devices can be queried with</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb4-1"><a href="#cb4-1"></a><span class="dt">int</span> oidnGetNumPhysicalDevices();</span></code></pre></div>
<p>The physical devices can be identified using IDs between 0 and (<code>oidnGetNumPhysicalDevices()</code> <span class="math inline">−</span> 1), and are ordered <em>approximately</em> from fastest to slowest (e.g., ID of 0 corresponds to the likely fastest physical device). Note that the reported number and order of physical devices may change between application runs, so no assumptions should be made about this list.</p>
<p>Parameters of these physical devices can be queried using</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb5-1"><a href="#cb5-1"></a><span class="dt">bool</span>         oidnGetPhysicalDeviceBool  (<span class="dt">int</span> physicalDeviceID, <span class="at">const</span> <span class="dt">char</span>* name);</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="dt">int</span>          oidnGetPhysicalDeviceInt   (<span class="dt">int</span> physicalDeviceID, <span class="at">const</span> <span class="dt">char</span>* name);</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="dt">unsigned</span> <span class="dt">int</span> oidnGetPhysicalDeviceUInt  (<span class="dt">int</span> physicalDeviceID, <span class="at">const</span> <span class="dt">char</span>* name);</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="at">const</span> <span class="dt">char</span>*  oidnGetPhysicalDeviceString(<span class="dt">int</span> physicalDeviceID, <span class="at">const</span> <span class="dt">char</span>* name);</span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="at">const</span> <span class="dt">void</span>*  oidnGetPhysicalDeviceData  (<span class="dt">int</span> physicalDeviceID, <span class="at">const</span> <span class="dt">char</span>* name,</span>
<span id="cb5-6"><a href="#cb5-6"></a>                                         <span class="dt">size_t</span>* byteSize);</span></code></pre></div>
<p>where <code>name</code> is the name of the parameter, and <code>byteSize</code> is the number of returned bytes for data parameters. The following parameters can be queried:</p>
<table style="width:98%;">
<caption>Constant parameters supported by physical devices.</caption>
<colgroup>
<col style="width: 13%" />
<col style="width: 28%" />
<col style="width: 55%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>type</code></td>
<td style="text-align: left;">device type as an <code>OIDNDeviceType</code> value</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>String</code></td>
<td style="text-align: left;"><code>name</code></td>
<td style="text-align: left;">name string</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Bool</code></td>
<td style="text-align: left;"><code>uuidSupported</code></td>
<td style="text-align: left;">device supports universally unique identifier (UUID)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Data</code></td>
<td style="text-align: left;"><code>uuid</code></td>
<td style="text-align: left;">opaque UUID (<code>OIDN_UUID_SIZE</code> bytes, exists only if <code>uuidSupported</code> is <code>true</code>)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Bool</code></td>
<td style="text-align: left;"><code>luidSupported</code></td>
<td style="text-align: left;">device supports locally unique identifier (UUID)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Data</code></td>
<td style="text-align: left;"><code>luid</code></td>
<td style="text-align: left;">opaque LUID (<code>OIDN_LUID_SIZE</code> bytes, exists only if <code>luidSupported</code> is <code>true</code>)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>UInt</code></td>
<td style="text-align: left;"><code>nodeMask</code></td>
<td style="text-align: left;">bitfield identifying the node within a linked device adapter corresponding to the device (exists only if <code>luidSupported</code> is <code>true</code>)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Bool</code></td>
<td style="text-align: left;"><code>pciAddressSupported</code></td>
<td style="text-align: left;">device supports PCI address</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>pciDomain</code></td>
<td style="text-align: left;">PCI domain (exists only if <code>pciAddressSupported</code> is <code>true</code>)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>pciBus</code></td>
<td style="text-align: left;">PCI bus (exists only if <code>pciAddressSupported</code> is <code>true</code>)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>pciDevice</code></td>
<td style="text-align: left;">PCI device (exists only if <code>pciAddressSupported</code> is <code>true</code>)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>pciFunction</code></td>
<td style="text-align: left;">PCI function (exists only if <code>pciAddressSupported</code> is <code>true</code>)</td>
</tr>
</tbody>
</table>
<h2 id="devices">Devices</h2>
<p>Open Image Denoise has a <em>logical</em> device concept as well, or simply referred to as <em>device</em>, which allows different components of the application to use the Open Image Denoise API without interfering with each other. Each physical device may be associated with one ore more logical devices. A basic way to create a device is by calling</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb6-1"><a href="#cb6-1"></a>OIDNDevice oidnNewDevice(OIDNDeviceType type);</span></code></pre></div>
<p>where the <code>type</code> enumeration maps to a specific device implementation, which can be one of the following:</p>
<table style="width:98%;">
<caption>Supported device types, i.e., valid constants of type <code>OIDNDeviceType</code>.</caption>
<colgroup>
<col style="width: 34%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_DEVICE_TYPE_DEFAULT</code></td>
<td style="text-align: left;">select the likely fastest device (same as physical device with ID 0)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_DEVICE_TYPE_CPU</code></td>
<td style="text-align: left;">CPU device</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_DEVICE_TYPE_SYCL</code></td>
<td style="text-align: left;">SYCL device (requires a supported Intel GPU)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_DEVICE_TYPE_CUDA</code></td>
<td style="text-align: left;">CUDA device (requires a supported NVIDIA GPU)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_DEVICE_TYPE_HIP</code></td>
<td style="text-align: left;">HIP device (requires a supported AMD GPU)</td>
</tr>
</tbody>
</table>
<p>If there are multiple supported devices of the specified type, an implementation-dependent default will be selected.</p>
<p>A device can be created by specifying a physical device ID as well using</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb7-1"><a href="#cb7-1"></a>OIDNDevice oidnNewDeviceByID(<span class="dt">int</span> physicalDeviceID);</span></code></pre></div>
<p>Applications can manually iterate over the list of physical devices and select from them based on their properties but there are also some built-in helper functions as well, which make creating a device by a particular physical device property easier:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb8-1"><a href="#cb8-1"></a>OIDNDevice oidnNewDeviceByUUID(<span class="at">const</span> <span class="dt">void</span>* uuid);</span>
<span id="cb8-2"><a href="#cb8-2"></a>OIDNDevice oidnNewDeviceByLUID(<span class="at">const</span> <span class="dt">void</span>* luid);</span>
<span id="cb8-3"><a href="#cb8-3"></a>OIDNDevice oidnNewDeviceByPCIAddress(<span class="dt">int</span> pciDomain, <span class="dt">int</span> pciBus, <span class="dt">int</span> pciDevice,</span>
<span id="cb8-4"><a href="#cb8-4"></a>                                     <span class="dt">int</span> pciFunction);</span></code></pre></div>
<p>These functions are particularly useful when the application needs interoperability with a graphics API (e.g. DX12, Vulkan). However, not all of these properties may be supported by the intended physical device (or drivers might even report inconsistent identifiers), so it is recommended to select by more than one property, if possible.</p>
<p>If the application requires interoperability with a particular compute API (SYCL, CUDA, HIP), it is recommended to use one of the following dedicated functions instead:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb9-1"><a href="#cb9-1"></a>OIDNDevice oidnNewSYCLDevice(<span class="at">const</span> sycl::queue* queues, <span class="dt">int</span> numQueues);</span>
<span id="cb9-2"><a href="#cb9-2"></a>OIDNDevice oidnNewCUDADevice(<span class="at">const</span> <span class="dt">int</span>* deviceIDs, <span class="at">const</span> <span class="dt">cudaStream_t</span>* streams,</span>
<span id="cb9-3"><a href="#cb9-3"></a>                             <span class="dt">int</span> numPairs);</span>
<span id="cb9-4"><a href="#cb9-4"></a>OIDNDevice oidnNewHIPDevice (<span class="at">const</span> <span class="dt">int</span>* deviceIDs, <span class="at">const</span> <span class="dt">hipStream_t</span>* streams,</span>
<span id="cb9-5"><a href="#cb9-5"></a>                             <span class="dt">int</span> numPairs);</span></code></pre></div>
<p>For SYCL, it is possible to pass one or more SYCL queues which will be used by Open Image Denoise for all device operations. This is useful when the application wants to use the same queues for both denoising and its own operations (e.g. rendering). Passing multiple queues is not intended to be used for different physical devices but just for a single SYCL root-device which consists of multiple sub-devices (e.g. Intel® Data Center GPU Max Series having multiple Xe-Stacks/tiles). The only supported SYCL backend is oneAPI Level Zero.</p>
<p>For CUDA and HIP, pairs of CUDA/HIP device IDs and corresponding streams can be specified but the current implementation supports only one pair. Negative device IDs correspond to the default device, and a <code>NULL</code> stream corresponds to the default stream on the corresponding device. Open Image Denoise automatically sets and restores the current CUDA/HIP device on the calling thread when necessary, thus the current device does not have to be changed manually by the application.</p>
<p>Once a device is created, you can call</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb10-1"><a href="#cb10-1"></a><span class="dt">bool</span> oidnGetDeviceBool(OIDNDevice device, <span class="at">const</span> <span class="dt">char</span>* name);</span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="dt">void</span> oidnSetDeviceBool(OIDNDevice device, <span class="at">const</span> <span class="dt">char</span>* name, <span class="dt">bool</span> value);</span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="dt">int</span>  oidnGetDeviceInt (OIDNDevice device, <span class="at">const</span> <span class="dt">char</span>* name);</span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="dt">void</span> oidnSetDeviceInt (OIDNDevice device, <span class="at">const</span> <span class="dt">char</span>* name, <span class="dt">int</span>  value);</span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="dt">int</span>  oidnGetDeviceUInt(OIDNDevice device, <span class="at">const</span> <span class="dt">char</span>* name);</span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="dt">void</span> oidnSetDeviceUInt(OIDNDevice device, <span class="at">const</span> <span class="dt">char</span>* name, <span class="dt">unsigned</span> <span class="dt">int</span> value);</span></code></pre></div>
<p>to set and get parameter values on the device. Note that some parameters are constants, thus trying to set them is an error. See the tables below for the parameters supported by devices.</p>
<table style="width:98%;">
<caption>Parameters supported by all devices.</caption>
<colgroup>
<col style="width: 10%" />
<col style="width: 30%" />
<col style="width: 14%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Name</th>
<th style="text-align: right;">Default</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>type</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">device type as an <code>OIDNDeviceType</code> value</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>version</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">combined version number (major.minor.patch) with two decimal digits per component</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>versionMajor</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">major version number</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>versionMinor</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">minor version number</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>versionPatch</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">patch version number</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Bool</code></td>
<td style="text-align: left;"><code>systemMemorySupported</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">device can directly access memory allocated with the system allocator (e.g. <code>malloc</code>)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Bool</code></td>
<td style="text-align: left;"><code>managedMemorySupported</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">device supports buffers created with managed storage (<code>OIDN_STORAGE_MANAGED</code>)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>externalMemoryTypes</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">bitfield of <code>OIDNExternalMemoryTypeFlag</code> values representing the external memory types supported by the device</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>verbose</code></td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">verbosity level of the console output between 0–4; when set to 0, no output is printed, when set to a higher level more output is printed</td>
</tr>
</tbody>
</table>
<table style="width:98%;">
<caption>Additional parameters supported only by CPU devices.</caption>
<colgroup>
<col style="width: 11%" />
<col style="width: 19%" />
<col style="width: 12%" />
<col style="width: 54%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Name</th>
<th style="text-align: right;">Default</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>numThreads</code></td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">maximum number of threads which the library should use; 0 will set it automatically to get the best performance</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Bool</code></td>
<td style="text-align: left;"><code>setAffinity</code></td>
<td style="text-align: right;"><code>true</code></td>
<td style="text-align: left;">enables thread affinitization (pinning software threads to hardware threads) if it is necessary for achieving optimal performance</td>
</tr>
</tbody>
</table>
<p>Note that the CPU device heavily relies on setting the thread affinities to achieve optimal performance, so it is highly recommended to leave this option enabled. However, this may interfere with the application if that also sets the thread affinities, potentially causing performance degradation. In such cases, the recommended solution is to either disable setting the affinities in the application or in Open Image Denoise, or to always set/reset the affinities before/after each parallel region in the application (e.g., if using TBB, with <code>tbb::task_arena</code> and <code>tbb::task_scheduler_observer</code>).</p>
<p>Once parameters are set on the created device, the device must be committed with</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb11-1"><a href="#cb11-1"></a><span class="dt">void</span> oidnCommitDevice(OIDNDevice device);</span></code></pre></div>
<p>This device can then be used to construct further objects, such as buffers and filters. Note that a device can be committed only once during its lifetime.</p>
<p>Some functions may execute asynchronously with respect to the host. The names of these functions are suffixed with <code>Async</code>. Asynchronous operations are executed <em>in order</em> on the device but may not block on the host. Eventually, it is necessary to wait for all asynchronous operations to complete, which can be done by calling</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb12-1"><a href="#cb12-1"></a><span class="dt">void</span> oidnSyncDevice(OIDNDevice device);</span></code></pre></div>
<p>Currently the CPU device does not support asynchronous execution, and thus the asynchronous versions of functions will block as well. However, <code>oidnSyncDevice</code> should be always called to ensure correctness on GPU devices too, which do support asynchronous execution.</p>
<p>Before the application exits, it should release all devices by invoking</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb13-1"><a href="#cb13-1"></a><span class="dt">void</span> oidnReleaseDevice(OIDNDevice device);</span></code></pre></div>
<p>Note that Open Image Denoise uses reference counting for all object types, so this function decreases the reference count of the device, and if the count reaches 0 the device will automatically get deleted. It is also possible to increase the reference count by calling</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb14-1"><a href="#cb14-1"></a><span class="dt">void</span> oidnRetainDevice(OIDNDevice device);</span></code></pre></div>
<p>An application should typically create only a single device object per physical device (one for <em>all</em> CPUs or one per GPU) as creation can be very expensive and additional device objects may incur a significant memory overhead. If required differently, it should only use a small number of device objects at any given time.</p>
<h3 id="error-handling">Error Handling</h3>
<p>Each user thread has its own error code per device. If an error occurs when calling an API function, this error code is set to the occurred error if it stores no previous error. The currently stored error can be queried by the application via</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb15-1"><a href="#cb15-1"></a>OIDNError oidnGetDeviceError(OIDNDevice device, <span class="at">const</span> <span class="dt">char</span>** outMessage);</span></code></pre></div>
<p>where <code>outMessage</code> can be a pointer to a C string which will be set to a more descriptive error message, or it can be <code>NULL</code>. This function also clears the error code, which assures that the returned error code is always the first error occurred since the last invocation of <code>oidnGetDeviceError</code> on the current thread. Note that the optionally returned error message string is valid only until the next invocation of the function.</p>
<p>Alternatively, the application can also register a callback function of type</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb16-1"><a href="#cb16-1"></a><span class="kw">typedef</span> <span class="dt">void</span> (*OIDNErrorFunction)(<span class="dt">void</span>* userPtr, OIDNError code, <span class="at">const</span> <span class="dt">char</span>* message);</span></code></pre></div>
<p>via</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb17-1"><a href="#cb17-1"></a><span class="dt">void</span> oidnSetDeviceErrorFunction(OIDNDevice device, OIDNErrorFunction func, <span class="dt">void</span>* userPtr);</span></code></pre></div>
<p>to get notified when errors occur. Only a single callback function can be registered per device, and further invocations overwrite the previously set callback function, which do <em>not</em> require also calling the <code>oidnCommitDevice</code> function. Passing <code>NULL</code> as function pointer disables the registered callback function. When the registered callback function is invoked, it gets passed the user-defined payload (<code>userPtr</code> argument as specified at registration time), the error code (<code>code</code> argument) of the occurred error, as well as a string (<code>message</code> argument) that further describes the error. The error code is always set even if an error callback function is registered. It is recommended to always set a error callback function, to detect all errors.</p>
<p>When the device construction fails, <code>oidnNewDevice</code> returns <code>NULL</code> as device. To detect the error code of a such failed device construction, pass <code>NULL</code> as device to the <code>oidnGetDeviceError</code> function. For all other invocations of <code>oidnGetDeviceError</code>, a proper device handle must be specified.</p>
<p>The following errors are currently used by Open Image Denoise:</p>
<table style="width:98%;">
<caption>Possible error codes, i.e., valid constants of type <code>OIDNError</code>.</caption>
<colgroup>
<col style="width: 45%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_ERROR_NONE</code></td>
<td style="text-align: left;">no error occurred</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_ERROR_UNKNOWN</code></td>
<td style="text-align: left;">an unknown error occurred</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_ERROR_INVALID_ARGUMENT</code></td>
<td style="text-align: left;">an invalid argument was specified</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_ERROR_INVALID_OPERATION</code></td>
<td style="text-align: left;">the operation is not allowed</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_ERROR_OUT_OF_MEMORY</code></td>
<td style="text-align: left;">not enough memory to execute the operation</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_ERROR_UNSUPPORTED_HARDWARE</code></td>
<td style="text-align: left;">the hardware (CPU/GPU) is not supported</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_ERROR_CANCELLED</code></td>
<td style="text-align: left;">the operation was cancelled by the user</td>
</tr>
</tbody>
</table>
<h3 id="environment-variables">Environment Variables</h3>
<p>Open Image Denoise supports environment variables for overriding certain settings at runtime, which can be useful for debugging and development:</p>
<table>
<caption>Environment variables supported by Open Image Denoise.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_DEFAULT_DEVICE</code></td>
<td style="text-align: left;">overrides what physical device to use with <code>OIDN_DEVICE_TYPE_DEFAULT</code>; can be <code>cpu</code>, <code>sycl</code>, <code>cuda</code>, <code>hip</code>, or a physical device ID</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_DEVICE_CPU</code></td>
<td style="text-align: left;">value of 0 disables CPU device support</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_DEVICE_SYCL</code></td>
<td style="text-align: left;">value of 0 disables SYCL device support</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_DEVICE_CUDA</code></td>
<td style="text-align: left;">value of 0 disables CUDA device support</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_DEVICE_HIP</code></td>
<td style="text-align: left;">value of 0 disables HIP device support</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_NUM_THREADS</code></td>
<td style="text-align: left;">overrides <code>numThreads</code> device parameter</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_SET_AFFINITY</code></td>
<td style="text-align: left;">overrides <code>setAffinity</code> device parameter</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_NUM_SUBDEVICES</code></td>
<td style="text-align: left;">overrides number of SYCL sub-devices to use (e.g. for Intel® Data Center GPU Max Series)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_VERBOSE</code></td>
<td style="text-align: left;">overrides <code>verbose</code> device parameter</td>
</tr>
</tbody>
</table>
<h2 id="buffers-1">Buffers</h2>
<p>Image data can be passed to Open Image Denoise either via pointers to memory allocated and managed by the user or by creating buffer objects. Regardless of which method is used, the data must be allocated in a way that it is accessible by the device (either CPU or GPU). Using buffers is typically the preferred approach because this ensures that the allocation requirements are fulfilled regardless of device type. To create a new data buffer with memory allocated and owned by the device, use</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb18-1"><a href="#cb18-1"></a>OIDNBuffer oidnNewBuffer(OIDNDevice device, <span class="dt">size_t</span> byteSize);</span></code></pre></div>
<p>The created buffer is bound to the specified device (<code>device</code> argument). The specified number of bytes (<code>byteSize</code>) are allocated at buffer construction time and deallocated when the buffer is destroyed. The memory is by default allocated as managed memory automatically migrated between host and device, if supported, or as pinned host memory otherwise.</p>
<p>If this default buffer allocation is not suitable, a buffer can be created with a manually specified storage mode as well:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb19-1"><a href="#cb19-1"></a>OIDNBuffer oidnNewBufferWithStorage(OIDNDevice device, <span class="dt">size_t</span> byteSize, OIDNStorage storage);</span></code></pre></div>
<p>The supported storage modes are the following:</p>
<table style="width:98%;">
<caption>Supported storage modes for buffers, i.e., valid constants of type <code>OIDNStorage</code>.</caption>
<colgroup>
<col style="width: 32%" />
<col style="width: 65%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_STORAGE_UNDEFINED</code></td>
<td style="text-align: left;">undefined storage mode</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_STORAGE_HOST</code></td>
<td style="text-align: left;">pinned host memory, accessible by both host and device</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_STORAGE_DEVICE</code></td>
<td style="text-align: left;">device memory, <em>not</em> accessible by the host</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_STORAGE_MANAGED</code></td>
<td style="text-align: left;">automatically migrated between host and device, accessible by both (<em>not</em> supported by all devices, <code>managedMemorySupported</code> device parameter must be checked before use)</td>
</tr>
</tbody>
</table>
<p>Note that the host and device storage modes are supported by all devices but managed storage is an optional feature. Before using managed storage, the <code>managedMemorySupported</code> device parameter should be queried.</p>
<p>It is also possible to create a “shared” data buffer with memory allocated and managed by the user with</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb20-1"><a href="#cb20-1"></a>OIDNBuffer oidnNewSharedBuffer(OIDNDevice device, <span class="dt">void</span>* devPtr, <span class="dt">size_t</span> byteSize);</span></code></pre></div>
<p>where <code>devPtr</code> points to user-managed device-accessible memory and <code>byteSize</code> is its size in bytes. At buffer construction time no buffer data is allocated, but the buffer data provided by the user is used. The buffer data must remain valid for as long as the buffer may be used, and the user is responsible to free the buffer data when no longer required. The user must also ensure that the memory is accessible by the device by using allocation functions supported by the device (e.g. <code>sycl::malloc_device</code>, <code>cudaMalloc</code>, <code>hipMalloc</code>).</p>
<p>Buffers can be also imported from graphics APIs as external memory, to avoid expensive copying of data through host memory. Different types of external memory can be imported from either POSIX file descriptors or Win32 handles using</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb21-1"><a href="#cb21-1"></a>OIDNBuffer oidnNewSharedBufferFromFD(OIDNDevice device,</span>
<span id="cb21-2"><a href="#cb21-2"></a>                                     OIDNExternalMemoryTypeFlag fdType,</span>
<span id="cb21-3"><a href="#cb21-3"></a>                                     <span class="dt">int</span> fd, <span class="dt">size_t</span> byteSize);</span>
<span id="cb21-4"><a href="#cb21-4"></a></span>
<span id="cb21-5"><a href="#cb21-5"></a>OIDNBuffer oidnNewSharedBufferFromWin32Handle(OIDNDevice device,</span>
<span id="cb21-6"><a href="#cb21-6"></a>                                              OIDNExternalMemoryTypeFlag handleType,</span>
<span id="cb21-7"><a href="#cb21-7"></a>                                              <span class="dt">void</span>* handle, <span class="at">const</span> <span class="dt">void</span>* name, <span class="dt">size_t</span> byteSize);</span></code></pre></div>
<p>Before exporting memory from the graphics API, the application should find a handle type which is supported by both the Open Image Denoise device (see <code>externalMemoryTypes</code> device parameter) and the graphics API. Note that different GPU vendors may support different handle types. To ensure compatibility with all device types, applications should support at least <code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_OPAQUE_FD</code> on Windows and both <code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_OPAQUE_FD</code> and <code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_DMA_BUF</code> on Linux. All possible external memory types are listed in the table below.</p>
<table style="width:98%;">
<caption>Supported external memory type flags, i.e., valid constants of type <code>OIDNExternalMemoryTypeFlag</code>.</caption>
<colgroup>
<col style="width: 56%" />
<col style="width: 41%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_NONE</code></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_OPAQUE_FD</code></td>
<td style="text-align: left;">opaque POSIX file descriptor handle (recommended on Linux)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_DMA_BUF</code></td>
<td style="text-align: left;">file descriptor handle for a Linux dma_buf (recommended on Linux)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_OPAQUE_WIN32</code></td>
<td style="text-align: left;">NT handle (recommended on Windows)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_OPAQUE_WIN32_KMT</code></td>
<td style="text-align: left;">global share (KMT) handle</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_D3D11_TEXTURE</code></td>
<td style="text-align: left;">NT handle returned by <code>IDXGIResource1::CreateSharedHandle</code> referring to a Direct3D 11 texture resource</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_D3D11_TEXTURE_KMT</code></td>
<td style="text-align: left;">global share (KMT) handle returned by <code>IDXGIResource::GetSharedHandle</code> referring to a Direct3D 11 texture resource</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_D3D11_RESOURCE</code></td>
<td style="text-align: left;">NT handle returned by <code>IDXGIResource1::CreateSharedHandle</code> referring to a Direct3D 11 resource</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_D3D11_RESOURCE_KMT</code></td>
<td style="text-align: left;">global share (KMT) handle returned by <code>IDXGIResource::GetSharedHandle</code> referring to a Direct3D 11 resource</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_D3D12_HEAP</code></td>
<td style="text-align: left;">NT handle returned by <code>ID3D12Device::CreateSharedHandle</code> referring to a Direct3D 12 heap resource</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_EXTERNAL_MEMORY_TYPE_FLAG_D3D12_RESOURCE</code></td>
<td style="text-align: left;">NT handle returned by <code>ID3D12Device::CreateSharedHandle</code> referring to a Direct3D 12 committed resource</td>
</tr>
</tbody>
</table>
<p>Similar to device objects, buffer objects are also reference-counted and can be retained and released by calling the following functions:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb22-1"><a href="#cb22-1"></a><span class="dt">void</span> oidnRetainBuffer (OIDNBuffer buffer);</span>
<span id="cb22-2"><a href="#cb22-2"></a><span class="dt">void</span> oidnReleaseBuffer(OIDNBuffer buffer);</span></code></pre></div>
<p>The size of in bytes and storage mode of the buffer can be queried using</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb23-1"><a href="#cb23-1"></a><span class="dt">size_t</span>      oidnGetBufferSize   (OIDNBuffer buffer);</span>
<span id="cb23-2"><a href="#cb23-2"></a>OIDNStorage oidnGetBufferStorage(OIDNBuffer buffer);</span></code></pre></div>
<p>It is possible to get a pointer directly to the buffer data, which is usually the preferred way to access the data stored in the buffer:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb24-1"><a href="#cb24-1"></a><span class="dt">void</span>* oidnGetBufferData(OIDNBuffer buffer);</span></code></pre></div>
<p>However, accessing the data on the host through this pointer is possible only if the buffer was created with a storage mode that enables this, i.e., any mode <em>except</em> <code>OIDN_STORAGE_DEVICE</code>. Note that a <code>NULL</code> pointer may be returned if the buffer is empty or getting a pointer to data with device storage is not supported by the device.</p>
<p>In some cases better performance can be achieved by using device storage for buffers. Such data can be accessed on the host by copying to/from host memory (including pageable system memory) using the following functions:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb25-1"><a href="#cb25-1"></a><span class="dt">void</span> oidnReadBuffer(OIDNBuffer buffer,</span>
<span id="cb25-2"><a href="#cb25-2"></a>                    <span class="dt">size_t</span> byteOffset, <span class="dt">size_t</span> byteSize, <span class="dt">void</span>* dstHostPtr);</span>
<span id="cb25-3"><a href="#cb25-3"></a></span>
<span id="cb25-4"><a href="#cb25-4"></a><span class="dt">void</span> oidnWriteBuffer(OIDNBuffer buffer,</span>
<span id="cb25-5"><a href="#cb25-5"></a>                     <span class="dt">size_t</span> byteOffset, <span class="dt">size_t</span> byteSize, <span class="at">const</span> <span class="dt">void</span>* srcHostPtr);</span></code></pre></div>
<p>These functions will always block until the read/write operation has been completed, which is often suboptimal. The following functions may execute the operation asynchronously if it is supported by the device (GPUs), or still block otherwise (CPUs):</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb26-1"><a href="#cb26-1"></a><span class="dt">void</span> oidnReadBufferAsync(OIDNBuffer buffer,</span>
<span id="cb26-2"><a href="#cb26-2"></a>                         <span class="dt">size_t</span> byteOffset, <span class="dt">size_t</span> byteSize, <span class="dt">void</span>* dstHostPtr);</span>
<span id="cb26-3"><a href="#cb26-3"></a></span>
<span id="cb26-4"><a href="#cb26-4"></a><span class="dt">void</span> oidnWriteBufferAsync(OIDNBuffer buffer,</span>
<span id="cb26-5"><a href="#cb26-5"></a>                          <span class="dt">size_t</span> byteOffset, <span class="dt">size_t</span> byteSize, <span class="at">const</span> <span class="dt">void</span>* srcHostPtr);</span></code></pre></div>
<p>When copying asynchronously, the user must ensure correct synchronization with the device by calling <code>oidnSyncDevice</code> before accessing the copied data or releasing the buffer. Failure to do so will result in undefined behavior.</p>
<h3 id="data-format">Data Format</h3>
<p>Buffers store opaque data and thus have no information about the type and format of the data. Other objects, e.g. filters, typically require specifying the format of the data stored in buffers or shared via pointers. This can be done using the <code>OIDNFormat</code> enumeration type:</p>
<table>
<caption>Supported data formats, i.e., valid constants of type <code>OIDNFormat</code>.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_FORMAT_UNDEFINED</code></td>
<td style="text-align: left;">undefined format</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_FORMAT_FLOAT</code></td>
<td style="text-align: left;">32-bit floating-point scalar</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_FORMAT_FLOAT[234]</code></td>
<td style="text-align: left;">32-bit floating-point [234]-element vector</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_FORMAT_HALF</code></td>
<td style="text-align: left;">16-bit floating-point scalar</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_FORMAT_HALF[234]</code></td>
<td style="text-align: left;">16-bit floating-point [234]-element vector</td>
</tr>
</tbody>
</table>
<h2 id="filters">Filters</h2>
<p>Filters are the main objects in Open Image Denoise that are responsible for the actual denoising. The library ships with a collection of filters which are optimized for different types of images and use cases. To create a filter object, call</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb27-1"><a href="#cb27-1"></a>OIDNFilter oidnNewFilter(OIDNDevice device, <span class="at">const</span> <span class="dt">char</span>* type);</span></code></pre></div>
<p>where <code>type</code> is the name of the filter type to create. The supported filter types are documented later in this section.</p>
<p>Creating filter objects can be very expensive, therefore it is <em>strongly</em> recommended to reuse the same filter for denoising as many images as possible, as long as the these images have the same same size, format, and features (i.e., only the memory locations and pixel values may be different). Otherwise (e.g. for images with different resolutions), reusing the same filter would not have any benefits.</p>
<p>Once created, filter objects can be retained and released with</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb28-1"><a href="#cb28-1"></a><span class="dt">void</span> oidnRetainFilter (OIDNFilter filter);</span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="dt">void</span> oidnReleaseFilter(OIDNFilter filter);</span></code></pre></div>
<p>After creating a filter, it needs to be set up by specifying the input and output images, and potentially setting other parameter values as well.</p>
<p>To set image parameters of a filter, you can use one of the following functions:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb29-1"><a href="#cb29-1"></a><span class="dt">void</span> oidnSetFilterImage(OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name,</span>
<span id="cb29-2"><a href="#cb29-2"></a>                        OIDNBuffer buffer, OIDNFormat format,</span>
<span id="cb29-3"><a href="#cb29-3"></a>                        <span class="dt">size_t</span> width, <span class="dt">size_t</span> height,</span>
<span id="cb29-4"><a href="#cb29-4"></a>                        <span class="dt">size_t</span> byteOffset,</span>
<span id="cb29-5"><a href="#cb29-5"></a>                        <span class="dt">size_t</span> pixelByteStride, <span class="dt">size_t</span> rowByteStride);</span>
<span id="cb29-6"><a href="#cb29-6"></a></span>
<span id="cb29-7"><a href="#cb29-7"></a><span class="dt">void</span> oidnSetSharedFilterImage(OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name,</span>
<span id="cb29-8"><a href="#cb29-8"></a>                              <span class="dt">void</span>* devPtr, OIDNFormat format,</span>
<span id="cb29-9"><a href="#cb29-9"></a>                              <span class="dt">size_t</span> width, <span class="dt">size_t</span> height,</span>
<span id="cb29-10"><a href="#cb29-10"></a>                              <span class="dt">size_t</span> byteOffset,</span>
<span id="cb29-11"><a href="#cb29-11"></a>                              <span class="dt">size_t</span> pixelByteStride, <span class="dt">size_t</span> rowByteStride);</span></code></pre></div>
<p>It is possible to specify either a data buffer object (<code>buffer</code> argument) with the <code>oidnSetFilterImage</code> function, or directly a pointer to user-managed device-accessible data (<code>devPtr</code> argument) with the <code>oidnSetSharedFilterImage</code> function. Regardless of whether a buffer or a pointer is specified, the data <em>must</em> be accessible to the device. The easiest way to guarantee this regardless of the device type (CPU or GPU) is using buffer objects.</p>
<p>In both cases, you must also specify the name of the image parameter to set (<code>name</code> argument, e.g. <code>"color"</code>, <code>"output"</code>), the pixel format (<code>format</code> argument), the width and height of the image in number of pixels (<code>width</code> and <code>height</code> arguments), the starting offset of the image data (<code>byteOffset</code> argument), the pixel stride (<code>pixelByteStride</code> argument) and the row stride (<code>rowByteStride</code> argument), in number of bytes.</p>
<p>If the pixels and/or rows are stored contiguously (tightly packed without any gaps), you can set <code>pixelByteStride</code> and/or <code>rowByteStride</code> to 0 to let the library compute the actual strides automatically, as a convenience.</p>
<p>Images support only <code>FLOAT</code> and <code>HALF</code> pixel formats with up to 3 channels. Custom image layouts with extra channels (e.g. alpha channel) or other data are supported as well by specifying a non-zero pixel stride. This way, expensive image layout conversion and copying can be avoided but the extra channels will be ignored by the filter. If these channels also need to be denoised, separate filters can be used.</p>
<p>To unset a previously set image parameter, returning it to a state as if it had not been set, call</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb30-1"><a href="#cb30-1"></a><span class="dt">void</span> oidnRemoveFilterImage(OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name);</span></code></pre></div>
<p>Some special data used by filters are opaque/untyped (e.g. trained model weights blobs), which can be specified with the <code>oidnSetSharedFilterData</code> function:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb31-1"><a href="#cb31-1"></a><span class="dt">void</span> oidnSetSharedFilterData(OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name,</span>
<span id="cb31-2"><a href="#cb31-2"></a>                             <span class="dt">void</span>* hostPtr, <span class="dt">size_t</span> byteSize);</span></code></pre></div>
<p>This data (<code>hostPtr</code>) must be accessible to the <em>host</em>, therefore system memory allocation is suitable (i.e., there is no reason to use buffer objects for allocation).</p>
<p>Modifying the contents of an opaque data parameter after setting it as a filter parameter is allowed but the filter needs to be notified that the data has been updated by calling</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb32-1"><a href="#cb32-1"></a><span class="dt">void</span> oidnUpdateFilterData(OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name);</span></code></pre></div>
<p>Unsetting an opaque data parameter can be performed with</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb33-1"><a href="#cb33-1"></a><span class="dt">void</span> oidnRemoveFilterData(OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name);</span></code></pre></div>
<p>Filters may have parameters other than buffers as well, which you can set and get using the following functions:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb34-1"><a href="#cb34-1"></a><span class="dt">bool</span>  oidnGetFilterBool (OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name);</span>
<span id="cb34-2"><a href="#cb34-2"></a><span class="dt">void</span>  oidnSetFilterBool (OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name, <span class="dt">bool</span>  value);</span>
<span id="cb34-3"><a href="#cb34-3"></a><span class="dt">int</span>   oidnGetFilterInt  (OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name);</span>
<span id="cb34-4"><a href="#cb34-4"></a><span class="dt">void</span>  oidnSetFilterInt  (OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name, <span class="dt">int</span>   value);</span>
<span id="cb34-5"><a href="#cb34-5"></a><span class="dt">float</span> oidnGetFilterFloat(OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name);</span>
<span id="cb34-6"><a href="#cb34-6"></a><span class="dt">void</span>  oidnSetFilterFloat(OIDNFilter filter, <span class="at">const</span> <span class="dt">char</span>* name, <span class="dt">float</span> value);</span></code></pre></div>
<p>Filters support a progress monitor callback mechanism that can be used to report progress of filter operations and to cancel them as well. Calling <code>oidnSetFilterProgressMonitorFunction</code> registers a progress monitor callback function (<code>func</code> argument) with payload (<code>userPtr</code> argument) for the specified filter (<code>filter</code> argument):</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb35-1"><a href="#cb35-1"></a><span class="kw">typedef</span> <span class="dt">bool</span> (*OIDNProgressMonitorFunction)(<span class="dt">void</span>* userPtr, <span class="dt">double</span> n);</span>
<span id="cb35-2"><a href="#cb35-2"></a></span>
<span id="cb35-3"><a href="#cb35-3"></a><span class="dt">void</span> oidnSetFilterProgressMonitorFunction(OIDNFilter filter,</span>
<span id="cb35-4"><a href="#cb35-4"></a>                                          OIDNProgressMonitorFunction func,</span>
<span id="cb35-5"><a href="#cb35-5"></a>                                          <span class="dt">void</span>* userPtr);</span></code></pre></div>
<p>Only a single callback function can be registered per filter, and further invocations overwrite the previously set callback function. Passing <code>NULL</code> as function pointer disables the registered callback function. Once registered, Open Image Denoise will invoke the callback function multiple times during filter operations, by passing the payload as set at registration time (<code>userPtr</code> argument), and a <code>double</code> in the range [0, 1] which estimates the progress of the operation (<code>n</code> argument). When returning <code>true</code> from the callback function, Open Image Denoise will continue the filter operation normally. When returning <code>false</code>, the library will attempt to cancel the filter operation as soon as possible, and if that is fulfilled, it will raise an <code>OIDN_ERROR_CANCELLED</code> error.</p>
<p>Please note that using a progress monitor callback function introduces some overhead, which may be significant on GPU devices, hurting performance. Therefore we recommend progress monitoring only for offline denoising, when denoising an image is expected to take several seconds.</p>
<p>After setting all necessary parameters for the filter, the changes must be committed by calling</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb36-1"><a href="#cb36-1"></a><span class="dt">void</span> oidnCommitFilter(OIDNFilter filter);</span></code></pre></div>
<p>The parameters can be updated after committing the filter, but it must be re-committed for any new changes to take effect. Committing major changes to the filter (e.g. setting new image parameters, changing the image resolution) can be expensive, and thus should not be done frequently (e.g. per frame).</p>
<p>Finally, an image can be filtered by executing the filter with</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb37-1"><a href="#cb37-1"></a><span class="dt">void</span> oidnExecuteFilter(OIDNFilter filter);</span></code></pre></div>
<p>which will read the input image data from the specified buffers and produce the denoised output image.</p>
<p>This function will always block until the filtering operation has been completed. The following function may execute the operation asynchronously if it is supported by the device (GPUs), or block otherwise (CPUs):</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb38-1"><a href="#cb38-1"></a><span class="dt">void</span> oidnExecuteFilterAsync(OIDNFilter filter);</span></code></pre></div>
<p>For filters created on a SYCL device it is also possible to specify dependent SYCL events (<code>depEvents</code> and <code>numDepEvents</code> arguments, may be <code>NULL</code>/0) and get a completion event as well (<code>doneEvent</code> argument, may be <code>NULL</code>):</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb39-1"><a href="#cb39-1"></a><span class="dt">void</span> oidnExecuteSYCLFilterAsync(OIDNFilter filter,</span>
<span id="cb39-2"><a href="#cb39-2"></a>                                <span class="at">const</span> sycl::event* depEvents, <span class="dt">int</span> numDepEvents,</span>
<span id="cb39-3"><a href="#cb39-3"></a>                                sycl::event* doneEvent);</span></code></pre></div>
<p>When filtering asynchronously, the user must ensure correct synchronization with the device by calling <code>oidnSyncDevice</code> before accessing the output image data or releasing the filter. Failure to do so will result in undefined behavior.</p>
<p>In the following we describe the different filters that are currently implemented in Open Image Denoise.</p>
<h3 id="rt">RT</h3>
<p>The <code>RT</code> (<strong>r</strong>ay <strong>t</strong>racing) filter is a generic ray tracing denoising filter which is suitable for denoising images rendered with Monte Carlo ray tracing methods like unidirectional and bidirectional path tracing. It supports depth of field and motion blur as well, but it is <em>not</em> temporally stable. The filter is based on a convolutional neural network (CNN) and comes with a set of pre-trained models that work well with a wide range of ray tracing based renderers and noise levels.</p>
<figure>
<img src="images/mazda_4spp_input.jpg" style="width:90.0%" alt="" /><figcaption>Example noisy beauty image rendered using unidirectional path tracing (4 samples per pixel). <em>Scene by Evermotion.</em></figcaption>
</figure>
<figure>
<img src="images/mazda_4spp_oidn.jpg" style="width:90.0%" alt="" /><figcaption>Example output beauty image denoised using prefiltered auxiliary feature images (albedo and normal) too.</figcaption>
</figure>
<p>For denoising <em>beauty</em> images, it accepts either a low dynamic range (LDR) or high dynamic range (HDR) image (<code>color</code>) as the main input image. In addition to this, it also accepts <em>auxiliary feature</em> images, <code>albedo</code> and <code>normal</code>, which are optional inputs that usually improve the denoising quality significantly, preserving more details.</p>
<p>It is possible to denoise auxiliary images as well, in which case only the respective auxiliary image has to be specified as input, instead of the beauty image. This can be done as a <em>prefiltering</em> step to further improve the quality of the denoised beauty image.</p>
<p>The <code>RT</code> filter has certain limitations regarding the supported input images. Most notably, it cannot denoise images that were not rendered with ray tracing. Another important limitation is related to anti-aliasing filters. Most renderers use a high-quality pixel reconstruction filter instead of a trivial box filter to minimize aliasing artifacts (e.g. Gaussian, Blackman-Harris). The <code>RT</code> filter does support such pixel filters but only if implemented with importance sampling. Weighted pixel sampling (sometimes called <em>splatting</em>) introduces correlation between neighboring pixels, which causes the denoising to fail (the noise will not be filtered), thus it is not supported.</p>
<p>The filter can be created by passing <code>"RT"</code> to the <code>oidnNewFilter</code> function as the filter type. The filter supports the parameters listed in the table below. All specified images must have the same dimensions. The output image can be one of the input images (i.e. in-place denoising is supported). See section <a href="#examples">Examples</a> for simple code snippets that demonstrate the usage of the filter.</p>
<table style="width:98%;">
<caption>Parameters supported by the <code>RT</code> filter.</caption>
<colgroup>
<col style="width: 11%" />
<col style="width: 20%" />
<col style="width: 14%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Name</th>
<th style="text-align: right;">Default</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>Image</code></td>
<td style="text-align: left;"><code>color</code></td>
<td style="text-align: right;"><em>optional</em></td>
<td style="text-align: left;">input beauty image (1–3 channels, LDR values in [0, 1] or HDR values in [0, +∞), values being interpreted such that, after scaling with the <code>inputScale</code> parameter, a value of 1 corresponds to a luminance level of 100 cd/m²)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Image</code></td>
<td style="text-align: left;"><code>albedo</code></td>
<td style="text-align: right;"><em>optional</em></td>
<td style="text-align: left;">input auxiliary image containing the albedo per pixel (1–3 channels, values in [0, 1])</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Image</code></td>
<td style="text-align: left;"><code>normal</code></td>
<td style="text-align: right;"><em>optional</em></td>
<td style="text-align: left;">input auxiliary image containing the shading normal per pixel (1–3 channels, world-space or view-space vectors with arbitrary length, values in [-1, 1])</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Image</code></td>
<td style="text-align: left;"><code>output</code></td>
<td style="text-align: right;"><em>required</em></td>
<td style="text-align: left;">output image (1–3 channels); can be one of the input images</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Bool</code></td>
<td style="text-align: left;"><code>hdr</code></td>
<td style="text-align: right;"><code>false</code></td>
<td style="text-align: left;">the main input image is HDR</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Bool</code></td>
<td style="text-align: left;"><code>srgb</code></td>
<td style="text-align: right;"><code>false</code></td>
<td style="text-align: left;">the main input image is encoded with the sRGB (or 2.2 gamma) curve (LDR only) or is linear; the output will be encoded with the same curve</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Float</code></td>
<td style="text-align: left;"><code>inputScale</code></td>
<td style="text-align: right;">NaN</td>
<td style="text-align: left;">scales values in the main input image before filtering, without scaling the output too, which can be used to map color or auxiliary feature values to the expected range, e.g. for mapping HDR values to physical units (which affects the quality of the output but <em>not</em> the range of the output values); if set to NaN, the scale is computed implicitly for HDR images or set to 1 otherwise</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Bool</code></td>
<td style="text-align: left;"><code>cleanAux</code></td>
<td style="text-align: right;"><code>false</code></td>
<td style="text-align: left;">the auxiliary feature (albedo, normal) images are noise-free; recommended for highest quality but should <em>not</em> be enabled for noisy auxiliary images to avoid residual noise</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>quality</code></td>
<td style="text-align: right;">high</td>
<td style="text-align: left;">image quality mode as an <code>OIDNQuality</code> value</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Data</code></td>
<td style="text-align: left;"><code>weights</code></td>
<td style="text-align: right;"><em>optional</em></td>
<td style="text-align: left;">trained model weights blob</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>maxMemoryMB</code></td>
<td style="text-align: right;">-1</td>
<td style="text-align: left;">if set to &gt;= 0, an attempt will be made to limit the memory usage below the specified amount in megabytes at the potential cost of slower performance but actual memory usage may be higher (the target may not be achievable or the device may not support this feature at all); otherwise memory usage will be limited to an unspecified device-dependent amount</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>tileAlignment</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">when manually denoising in tiles, the tile size and offsets should be multiples of this amount of pixels to avoid artifacts; when denoising HDR images <code>inputScale</code> <em>must</em> be set by the user to avoid seam artifacts</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>tileOverlap</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">when manually denoising in tiles, the tiles should overlap by this amount of pixels</td>
</tr>
</tbody>
</table>
<p>Using auxiliary feature images like albedo and normal helps preserving fine details and textures in the image thus can significantly improve denoising quality. These images should typically contain feature values for the first hit (i.e. the surface which is directly visible) per pixel. This works well for most surfaces but does not provide any benefits for reflections and objects visible through transparent surfaces (compared to just using the color as input). However, this issue can be usually fixed by storing feature values for a subsequent hit (i.e. the reflection and/or refraction) instead of the first hit. For example, it usually works well to follow perfect specular (<em>delta</em>) paths and store features for the first diffuse or glossy surface hit instead (e.g. for perfect specular dielectrics and mirrors). This can greatly improve the quality of reflections and transmission. We will describe this approach in more detail in the following subsections.</p>
<p>The auxiliary feature images should be as noise-free as possible. It is not a strict requirement but too much noise in the feature images may cause residual noise in the output. Ideally, these should be completely noise-free. If this is the case, this should be hinted to the filter using the <code>cleanAux</code> parameter to ensure the highest possible image quality. But this parameter should be used with care: if enabled, any noise present in the auxiliary images will end up in the denoised image as well, as residual noise. Thus, <code>cleanAux</code> should be enabled only if the auxiliary images are guaranteed to be noise-free.</p>
<p>Usually it is difficult to provide clean feature images, and some residual noise might be present in the output even with <code>cleanAux</code> being disabled. To eliminate this noise and to even improve the sharpness of texture details, the auxiliary images should be first denoised in a prefiltering step, as mentioned earlier. Then, these denoised auxiliary images could be used for denoising the beauty image. Since these are now noise-free, the <code>cleanAux</code> parameter should be enabled. See section <a href="#denoising-with-prefiltering-c11-api">Denoising with prefiltering (C++11 API)</a> for a simple code example. Prefiltering makes denoising much more expensive but if there are multiple color AOVs to denoise, the prefiltered auxiliary images can be reused for denoising multiple AOVs, amortizing the cost of the prefiltering step.</p>
<p>Thus, for final-frame denoising, where the best possible image quality is required, it is recommended to prefilter the auxiliary features if they are noisy and enable the <code>cleanAux</code> parameter. Denoising with noisy auxiliary features should be reserved for previews and interactive rendering.</p>
<p>All auxiliary images should use the same pixel reconstruction filter as the beauty image. Using a properly anti-aliased beauty image but aliased albedo or normal images will likely introduce artifacts around edges.</p>
<h4 id="albedos">Albedos</h4>
<p>The albedo image is the feature image that usually provides the biggest quality improvement. It should contain the approximate color of the surfaces independent of illumination and viewing angle.</p>
<figure>
<img src="images/mazda_firsthit_512spp_albedo.jpg" style="width:90.0%" alt="" /><figcaption>Example albedo image obtained using the first hit. Note that the albedos of all transparent surfaces are 1.</figcaption>
</figure>
<figure>
<img src="images/mazda_nondeltahit_512spp_albedo.jpg" style="width:90.0%" alt="" /><figcaption>Example albedo image obtained using the first diffuse or glossy (non-delta) hit. Note that the albedos of perfect specular (delta) transparent surfaces are computed as the Fresnel blend of the reflected and transmitted albedos.</figcaption>
</figure>
<p>For simple matte surfaces this means using the diffuse color/texture as the albedo. For other, more complex surfaces it is not always obvious what is the best way to compute the albedo, but the denoising filter is flexible to a certain extent and works well with differently computed albedos. Thus it is not necessary to compute the strict, exact albedo values but must be always between 0 and 1.</p>
<p>For metallic surfaces the albedo should be either the reflectivity at normal incidence (e.g. from the artist friendly metallic Fresnel model) or the average reflectivity; or if these are constant (not textured) or unknown, the albedo can be simply 1 as well.</p>
<p>The albedo for dielectric surfaces (e.g. glass) should be either 1 or, if the surface is perfect specular (i.e. has a delta BSDF), the Fresnel blend of the reflected and transmitted albedos. The latter usually works better but only if it does not introduce too much noise or the albedo is prefiltered. If noise is an issue, we recommend to split the path into a reflected and a transmitted path at the first hit, and perhaps fall back to an albedo of 1 for subsequent dielectric hits. The reflected albedo in itself can be used for mirror-like surfaces as well.</p>
<p>The albedo for layered surfaces can be computed as the weighted sum of the albedos of the individual layers. Non-absorbing clear coat layers can be simply ignored (or the albedo of the perfect specular reflection can be used as well) but absorption should be taken into account.</p>
<h4 id="normals">Normals</h4>
<p>The normal image should contain the shading normals of the surfaces either in world-space or view-space. It is recommended to include normal maps to preserve as much detail as possible.</p>
<figure>
<img src="images/mazda_firsthit_512spp_normal.jpg" style="width:90.0%" alt="" /><figcaption>Example normal image obtained using the first hit (the values are actually in [−1, 1] but were mapped to [0, 1] for illustration purposes).</figcaption>
</figure>
<figure>
<img src="images/mazda_nondeltahit_512spp_normal.jpg" style="width:90.0%" alt="" /><figcaption>Example normal image obtained using the first diffuse or glossy (non-delta) hit. Note that the normals of perfect specular (delta) transparent surfaces are computed as the Fresnel blend of the reflected and transmitted normals.</figcaption>
</figure>
<p>Just like any other input image, the normal image should be anti-aliased (i.e. by accumulating the normalized normals per pixel). The final accumulated normals do not have to be normalized but must be in the [-1, 1] range (i.e. normals mapped to [0, 1] are <em>not</em> acceptable and must be remapped to [−1, 1]).</p>
<p>Similar to the albedo, the normal can be stored for either the first or a subsequent hit (if the first hit has a perfect specular/delta BSDF).</p>
<h4 id="quality">Quality</h4>
<p>The filter supports setting an image quality mode, which determines whether to favor quality, performance, or have a balanced solution between the two. The supported quality modes are listed in the following table.</p>
<table>
<caption>Supported image quality modes, i.e., valid constants of type <code>OIDNQuality</code>.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_QUALITY_DEFAULT</code></td>
<td style="text-align: left;">default quality</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>OIDN_QUALITY_BALANCED</code></td>
<td style="text-align: left;">balanced quality/performance (for interactive/real-time rendering)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>OIDN_QUALITY_HIGH</code></td>
<td style="text-align: left;">high quality (for final-frame rendering); <em>default</em></td>
</tr>
</tbody>
</table>
<p>By default filtering is performed in high quality mode, which is recommended for final-frame rendering. Using this setting the results have the same high quality regardless of what kind of device (CPU or GPU) is used. However, due to significant hardware architecture differences between devices, there might be small numerical differences between the produced outputs.</p>
<p>The balanced quality mode is very close in image quality to the high quality mode except that lower numerical precision is used, if this is supported by the device. This may result in significantly higher performance on some devices but on others there might be no difference at all due to hardware specifics. This mode is recommended for interactive and real-time rendering.</p>
<p>Note that in balanced quality mode a higher variation in image quality should be expected across devices.</p>
<h4 id="weights">Weights</h4>
<p>Instead of using the built-in trained models for filtering, it is also possible to specify user-trained models at runtime. This can be achieved by passing the model <em>weights</em> blob corresponding to the specified set of features and other filter parameters, produced by the included training tool. See Section <a href="#training">Training</a> for details.</p>
<h3 id="rtlightmap">RTLightmap</h3>
<p>The <code>RTLightmap</code> filter is a variant of the <code>RT</code> filter optimized for denoising HDR and normalized directional (e.g. spherical harmonics) lightmaps. It does not support LDR images.</p>
<p>The filter can be created by passing <code>"RTLightmap"</code> to the <code>oidnNewFilter</code> function as the filter type. The filter supports the following parameters:</p>
<table style="width:98%;">
<caption>Parameters supported by the <code>RTLightmap</code> filter.</caption>
<colgroup>
<col style="width: 11%" />
<col style="width: 20%" />
<col style="width: 14%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Name</th>
<th style="text-align: right;">Default</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>Image</code></td>
<td style="text-align: left;"><code>color</code></td>
<td style="text-align: right;"><em>required</em></td>
<td style="text-align: left;">input beauty image (1–3 channels, HDR values in [0, +∞), interpreted such that, after scaling with the <code>inputScale</code> parameter, a value of 1 corresponds to a luminance level of 100 cd/m²; directional values in [-1, 1])</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Image</code></td>
<td style="text-align: left;"><code>output</code></td>
<td style="text-align: right;"><em>required</em></td>
<td style="text-align: left;">output image (1–3 channels); can be one of the input images</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Bool</code></td>
<td style="text-align: left;"><code>directional</code></td>
<td style="text-align: right;"><code>false</code></td>
<td style="text-align: left;">whether the input contains normalized coefficients (in [-1, 1]) of a directional lightmap (e.g. normalized L1 or higher spherical harmonics band with the L0 band divided out); if the range of the coefficients is different from [-1, 1], the <code>inputScale</code> parameter can be used to adjust the range without changing the stored values</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Float</code></td>
<td style="text-align: left;"><code>inputScale</code></td>
<td style="text-align: right;">NaN</td>
<td style="text-align: left;">scales input color values before filtering, without scaling the output too, which can be used to map color values to the expected range, e.g. for mapping HDR values to physical units (which affects the quality of the output but <em>not</em> the range of the output values); if set to NaN, the scale is computed implicitly for HDR images or set to 1 otherwise</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>quality</code></td>
<td style="text-align: right;">high</td>
<td style="text-align: left;">image quality mode as an <code>OIDNQuality</code> value</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Data</code></td>
<td style="text-align: left;"><code>weights</code></td>
<td style="text-align: right;"><em>optional</em></td>
<td style="text-align: left;">trained model weights blob</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>maxMemoryMB</code></td>
<td style="text-align: right;">-1</td>
<td style="text-align: left;">if set to &gt;= 0, an attempt will be made to limit the memory usage below the specified amount in megabytes at the potential cost of slower performance but actual memory usage may be higher (the target may not be achievable or the device may not support this feature at all); otherwise memory usage will be limited to an unspecified device-dependent amount</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>tileAlignment</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">when manually denoising in tiles, the tile size and offsets should be multiples of this amount of pixels to avoid artifacts; when denoising HDR images <code>inputScale</code> <em>must</em> be set by the user to avoid seam artifacts</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Int</code></td>
<td style="text-align: left;"><code>tileOverlap</code></td>
<td style="text-align: right;"><em>constant</em></td>
<td style="text-align: left;">when manually denoising in tiles, the tiles should overlap by this amount of pixels</td>
</tr>
</tbody>
</table>
<h1 id="examples-1">Examples</h1>
<p>Intel Open Image Denoise ships with a couple of simple example applications.</p>
<h2 id="oidndenoise">oidnDenoise</h2>
<p><code>oidnDenoise</code> is a minimal working example demonstrating how to use Intel Open Image Denoise, which can be found at <code>apps/oidnDenoise.cpp</code>. It uses the C++11 convenience wrappers of the C99 API.</p>
<p>This example is a simple command-line application that denoises the provided image, which can optionally have auxiliary feature images as well (e.g. albedo and normal). By default the images must be stored in the <a href="http://www.pauldebevec.com/Research/HDR/PFM/">Portable FloatMap</a> (PFM) format, and the color values must be encoded in little-endian format. To enable other image formats (e.g. OpenEXR, PNG) as well, the project has to be rebuilt with OpenImageIO support enabled.</p>
<p>Running <code>oidnDenoise</code> without any arguments or the <code>-h</code> argument will bring up a list of command-line options.</p>
<h2 id="oidnbenchmark">oidnBenchmark</h2>
<p><code>oidnBenchmark</code> is a basic command-line benchmarking application for measuring denoising speed, which can be found at <code>apps/oidnBenchmark.cpp</code>.</p>
<p>Running <code>oidnBenchmark</code> with the <code>-h</code> argument will bring up a list of command-line options.</p>
<h1 id="training">Training</h1>
<p>The Intel Open Image Denoise source distribution includes a Python-based neural network training toolkit (located in the <code>training</code> directory), which can be used to train the denoising filter models with image datasets provided by the user. This is an advanced feature of the library which usage requires some background knowledge of machine learning and basic familiarity with deep learning frameworks and toolkits (e.g. PyTorch or TensorFlow, TensorBoard).</p>
<p>The training toolkit consists of the following command-line scripts:</p>
<ul>
<li><p><code>preprocess.py</code>: Preprocesses training and validation datasets.</p></li>
<li><p><code>train.py</code>: Trains a model using preprocessed datasets.</p></li>
<li><p><code>infer.py</code>: Performs inference on a dataset using the specified training result.</p></li>
<li><p><code>export.py</code>: Exports a training result to the runtime model weights format.</p></li>
<li><p><code>find_lr.py</code>: Tool for finding the optimal minimum and maximum learning rates.</p></li>
<li><p><code>visualize.py</code>: Invokes TensorBoard for visualizing statistics of a training result.</p></li>
<li><p><code>split_exr.py</code>: Splits a multi-channel EXR image into multiple feature images.</p></li>
<li><p><code>convert_image.py</code>: Converts a feature image to a different image format.</p></li>
<li><p><code>compare_image.py</code>: Compares two feature images using the specified quality metrics.</p></li>
</ul>
<h2 id="prerequisites">Prerequisites</h2>
<p>Before you can run the training toolkit you need the following prerequisites:</p>
<ul>
<li><p>Linux (other operating systems are currently not supported)</p></li>
<li><p>Python 3.7 or later</p></li>
<li><p><a href="https://pytorch.org/">PyTorch</a> 1.8 or later</p></li>
<li><p><a href="https://numpy.org/">NumPy</a> 1.19 or later</p></li>
<li><p><a href="http://openimageio.org/">OpenImageIO</a> 2.1 or later</p></li>
<li><p><a href="https://www.tensorflow.org/tensorboard">TensorBoard</a> 2.4 or later (<em>optional</em>)</p></li>
</ul>
<h2 id="devices-1">Devices</h2>
<p>Most scripts in the training toolkit support selecting what kind of device (e.g. CPU, GPU) to use for the computations (<code>--device</code> or <code>-d</code> option). If multiple devices of the same kind are available (e.g. multiple GPUs), the user can specify which one of these to use (<code>--device_id</code> or <code>-k</code> option). Additionally, some scripts, like <code>train.py</code>, support data-parallel execution on multiple devices for faster performance (<code>--num_devices</code> or <code>-n</code> option).</p>
<h2 id="datasets">Datasets</h2>
<p>A dataset should consist of a collection of noisy and corresponding noise-free reference images. It is possible to have more than one noisy version of the same image in the dataset, e.g. rendered at different samples per pixel and/or using different seeds.</p>
<p>The training toolkit expects to have all datasets (e.g. training, validation) in the same parent directory (e.g. <code>data</code>). Each dataset is stored in its own subdirectory (e.g. <code>train</code>, <code>valid</code>), which can have an arbitrary name.</p>
<p>The images must be stored in <a href="https://www.openexr.com/">OpenEXR</a> format (<code>.exr</code> files), and the filenames must have a specific format but the files can be stored in an arbitrary directory structure inside the dataset directory. The only restriction is that all versions of an image (noisy images and the reference image) must be located in the same subdirectory. Each feature of an image (e.g. color, albedo) must be stored in a separate image file, i.e. multi-channel EXR image files are not supported. If you have multi-channel EXRs, you can split them into separate images per feature using the included <code>split_exr.py</code> tool.</p>
<p>An image filename must consist of a base name, a suffix with the number of samples per pixel or whether it is the reference image (e.g. <code>_0128spp</code>, <code>_ref</code>), the feature type extension (e.g. <code>.hdr</code>, <code>.alb</code>), and the image format extension (<code>.exr</code>). The exact filename format as a regular expression is the following:</p>
<pre class="regexp"><code>.+_([0-9]+(spp)?|ref|reference|gt|target)\.(hdr|ldr|sh1[xyz]|alb|nrm)\.exr</code></pre>
<p>The number of samples per pixel should be padded with leading zeros to have a fixed number of digits. If the reference image is not explicitly named as such (i.e. has the number of samples instead), the image with the most samples per pixel will be considered the reference.</p>
<p>The following image features are supported:</p>
<table>
<caption>Image features supported by the training toolkit.</caption>
<thead>
<tr class="header">
<th>Feature</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Channels</th>
<th style="text-align: left;">File extension</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>hdr</code></td>
<td style="text-align: left;">color (HDR)</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;"><code>.hdr.exr</code></td>
</tr>
<tr class="even">
<td><code>ldr</code></td>
<td style="text-align: left;">color (LDR)</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;"><code>.ldr.exr</code></td>
</tr>
<tr class="odd">
<td><code>sh1</code></td>
<td style="text-align: left;">color (normalized L1 spherical harmonics)</td>
<td style="text-align: left;">3 × 3 images</td>
<td style="text-align: left;"><code>.sh1x.exr</code>, <code>.sh1y.exr</code>, <code>.sh1z.exr</code></td>
</tr>
<tr class="even">
<td><code>alb</code></td>
<td style="text-align: left;">albedo</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;"><code>.alb.exr</code></td>
</tr>
<tr class="odd">
<td><code>nrm</code></td>
<td style="text-align: left;">normal</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;"><code>.nrm.exr</code></td>
</tr>
</tbody>
</table>
<p>The following directory tree demonstrates an example root dataset directory (<code>data</code>) containing one dataset (<code>rt_train</code>) with HDR color and albedo feature images:</p>
<pre><code>data
`-- rt_train
    |-- scene1
    |   |-- view1_0001.alb.exr
    |   |-- view1_0001.hdr.exr
    |   |-- view1_0004.alb.exr
    |   |-- view1_0004.hdr.exr
    |   |-- view1_8192.alb.exr
    |   |-- view1_8192.hdr.exr
    |   |-- view2_0001.alb.exr
    |   |-- view2_0001.hdr.exr
    |   |-- view2_8192.alb.exr
    |   `-- view2_8192.hdr.exr
    |-- scene2_000008spp.alb.exr
    |-- scene2_000008spp.hdr.exr
    |-- scene2_000064spp.alb.exr
    |-- scene2_000064spp.hdr.exr
    |-- scene2_reference.alb.exr
    `-- scene2_reference.hdr.exr</code></pre>
<h2 id="preprocessing-preprocess.py">Preprocessing (preprocess.py)</h2>
<p>Training and validation datasets can be used only after preprocessing them using the <code>preprocess.py</code> script. This will convert the specified training (<code>--train_data</code> or <code>-t</code> option) and validation datasets (<code>--valid_data</code> or <code>-v</code> option) located in the root dataset directory (<code>--data_dir</code> or <code>-D</code> option) to a format that can be loaded more efficiently during training. All preprocessed datasets will be stored in a root preprocessed dataset directory (<code>--preproc_dir</code> or <code>-P</code> option).</p>
<p>The preprocessing script requires the set of image features to include in the preprocessed dataset as command-line arguments. Only these specified features will be available for training but it is not required to use all of them at the same time. Thus, a single preprocessed dataset can be reused for training multiple models with different combinations of the preprocessed features.</p>
<p>By default, all input features are assumed to be noisy, including the auxiliary features (e.g. albedo, normal), each having versions at different samples per pixel. However, it is also possible to train with noise-free auxiliary features, in which case the reference auxiliary features are used instead of the various noisy ones (<code>--clean_aux</code> option).</p>
<p>Preprocessing also depends on the filter that will be trained (e.g. determines which HDR/LDR transfer function has to be used), which should be also specified (<code>--filter</code> or <code>-f</code> option). The alternative is to manually specify the transfer function (<code>--transfer</code> or <code>-x</code> option) and other filter-specific parameters, which could be useful for training custom filters.</p>
<p>For example, to preprocess the training and validation datasets (<code>rt_train</code> and <code>rt_valid</code>) with HDR color, albedo, and normal image features, for training the <code>RT</code> filter, the following command can be used:</p>
<pre><code>./preprocess.py hdr alb nrm --filter RT --train_data rt_train --valid_data rt_valid</code></pre>
<p>It is possible to preprocess the same dataset multiple times, with possibly different combinations of features and options. The training script will use the most suitable and most recent preprocessed version depending on the training parameters.</p>
<p>For more details about using the preprocessing script, including other options, please have a look at the help message:</p>
<pre><code>./preprocess.py -h</code></pre>
<h2 id="training-train.py">Training (train.py)</h2>
<p>The filters require separate trained models for each supported combination of input features. Thus, depending on which combinations of features the user wants to support for a particular filter, one or more models have to be trained.</p>
<p>After preprocessing the datasets, it is possible to start training a model using the <code>train.py</code> script. Similar to the preprocessing script, the input features must be specified (could be a subset of the preprocessed features), and the dataset names, directory paths, and the filter can be also passed.</p>
<p>The tool will produce a training <em>result</em>, the name of which can be either specified (<code>--result</code> or <code>-r</code> option) or automatically generated (by default). Each result is stored in its own subdirectory, and these are located in a common parent directory (<code>--results_dir</code> or <code>-R</code> option). If a training result already exists, the tool will resume training that result from the latest checkpoint.</p>
<p>The default training hyperparameters should work reasonably well in general, but some adjustments might be necessary for certain datasets to attain optimal performance, most importantly: the number of epochs (<code>--num_epochs</code> or <code>-e</code> option), the global mini-batch size (<code>--batch_size</code> or <code>-b</code> option), and the learning rate. The training tool uses a one-cycle learning rate schedule with cosine annealing, which can be configured by setting the base learning rate (<code>--learning_rate</code> or <code>--lr</code> option), the maximum learning rate (<code>--max_learning_rate</code> or <code>--max_lr</code> option), and the percentage of the cycle spent increasing the learning rate (<code>--learning_rate_warmup</code> or <code>--lr_warmup</code> option).</p>
<p>Example usage:</p>
<pre><code>./train.py hdr alb --filter RT --train_data rt_train --valid_data rt_valid --result rt_hdr_alb</code></pre>
<p>For finding the optimal learning rate range, we recommend using the included <code>find_lr.py</code> script, which trains one epoch using an increasing learning rate and logs the resulting losses in a comma-separated values (CSV) file. Plotting the loss curve can show when the model starts to learn (the base learning rate) and when it starts to diverge (the maximum learning rate).</p>
<p>The model is evaluated with the validation dataset at regular intervals (<code>--num_valid_epochs</code> option), and checkpoints are also regularly created (<code>--num_save_epochs</code> option) to save training progress. Also, some statistics are logged (e.g. training and validation losses, learning rate) per epoch, which can be later visualized with TensorBoard by running the <code>visualize.py</code> script, e.g.:</p>
<pre><code>./visualize.py --result rt_hdr_alb</code></pre>
<p>Training is performed with mixed precision (FP16 and FP32) by default, if it supported by the hardware, which makes training faster and use less memory. However, in some rare cases this might cause some convergence issues. The training precision can be manually set to FP32 if necessary (<code>--precision</code> or <code>-p</code> option).</p>
<h2 id="inference-infer.py">Inference (infer.py)</h2>
<p>A training result can be tested by performing inference on an image dataset (<code>--input_data</code> or <code>-i</code> option) using the <code>infer.py</code> script. The dataset does <em>not</em> have to be preprocessed. In addition to the result to use, it is possible to specify which checkpoint to load as well (<code>-e</code> or <code>--num_epochs</code> option). By default the latest checkpoint is loaded.</p>
<p>The tool saves the output images in a separate directory (<code>--output_dir</code> or <code>-O</code> option) in the requested formats (<code>--format</code> or <code>-F</code> option). It also evaluates a set of image quality metrics (<code>--metric</code> or <code>-M</code> option), e.g. PSNR, SSIM, for images that have reference images available. All metrics are computed in tonemapped non-linear sRGB space. Thus, HDR images are first tonemapped (with Naughty Dog’s Filmic Tonemapper from John Hable’s <em>Uncharted 2: HDR Lighting</em> presentation) and converted to sRGB before evaluating the metrics.</p>
<p>Example usage:</p>
<pre><code>./infer.py --result rt_hdr_alb --input_data rt_test --format exr png --metric ssim</code></pre>
<p>The inference tool supports prefiltering of auxiliary features as well, which can be performed by specifying the list of training results for each feature to prefilter (<code>--aux_results</code> or <code>-a</code> option). This is primarily useful for evaluating the quality of models trained with clean auxiliary features.</p>
<h2 id="exporting-results-export.py">Exporting Results (export.py)</h2>
<p>The training result produced by the <code>train.py</code> script cannot be immediately used by the main library. It has to be first exported to the runtime model weights format, a <em>Tensor Archive</em> (TZA) file. Running the <code>export.py</code> script for a training result (and optionally a checkpoint epoch) will create a binary <code>.tza</code> file in the directory of the result, which can be either used at runtime through the API or it can be included in the library build by replacing one of the built-in weights files.</p>
<p>Example usage:</p>
<pre><code>./export.py --result rt_hdr_alb</code></pre>
<h2 id="image-conversion-and-comparison">Image Conversion and Comparison</h2>
<p>In addition to the already mentioned <code>split_exr.py</code> script, the toolkit contains a few other image utilities as well.</p>
<p><code>convert_image.py</code> converts a feature image to a different image format (and/or a different feature, e.g. HDR color to LDR), performing tonemapping and other transforms as well if needed. For HDR images the exposure can be adjusted by passing a linear exposure scale (<code>--exposure</code> or <code>-E</code> option). Example usage:</p>
<pre><code>./convert_image.py view1_0004.hdr.exr view1_0004.png --exposure 2.5</code></pre>
<p>The <code>compare_image.py</code> script compares two feature images (preferably having the dataset filename format to correctly detect the feature) using the specified image quality metrics, similar to the <code>infer.py</code> tool. Example usage:</p>
<pre><code>./compare_image.py view1_0004.hdr.exr view1_8192.hdr.exr --exposure 2.5 --metric mse ssim</code></pre>

      </div>
    </div>

      <div id="footer">
        © 2018–2023 Intel Corporation <a href="legal.html">Disclaimer and Legal Information</a>
        <a href="https://www.intel.com/privacy">Privacy</a>
      </div>
  </body>
</html>
